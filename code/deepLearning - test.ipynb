{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af805449-d5f3-45a7-aa8a-0b8627bdc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21489306-cedb-4b59-867e-4c4fc7bbce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_difference(df):\n",
    "    diff_columns = lie_data.columns[1:28]\n",
    "    grouped = df.groupby(['question_no'])\n",
    "    df[diff_columns] = grouped[diff_columns].diff()\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "def group_split(X, y, group, train_size = 0.8):\n",
    "    splitter = GroupShuffleSplit(train_size = train_size)\n",
    "    train, test = next(splitter.split(X, y, groups = group))\n",
    "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdac5a-1b65-4fe8-acd7-35c50a825130",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b214fae-1f67-4e8e-ae89-ae839d3e36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_data = pd.DataFrame([])\n",
    "\n",
    "for i in range(1,4):\n",
    "    candidate = pd.read_csv(rf'.\\data\\VID{i}_data.csv')\n",
    "    # change Q\n",
    "    candidate['question_no'] = i + 0.01*candidate['question_no']\n",
    "    \n",
    "    lie_data  = pd.concat( [lie_data, candidate] )\n",
    "\n",
    "print(lie_data.shape)\n",
    "lie_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bbfe986-57f2-488c-908b-89da0aa2da46",
   "metadata": {},
   "source": [
    "## HIDDEN\n",
    "consecutive_difference(lie_data)\n",
    "print(lie_data.shape)\n",
    "lie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ec4ff-770c-49a0-a3e9-defd886487d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lie_data = lie_data.sample(frac=1)\n",
    "\n",
    "X = lie_data.copy().dropna()\n",
    "questio_no = X['question_no']\n",
    "\n",
    "X = X.drop(['frame', 'question_no'], axis=1)\n",
    "y = X.pop('TRUTH')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = group_split(X, y, questio_no)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ec939-a0b1-45b2-a1a9-5392b0b8e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\tDATABASE\")\n",
    "print(X.shape)\n",
    "print(\"TRUE :\", lie_data[ lie_data['TRUTH'] == 1 ].shape)\n",
    "print(\"FALSE:\", lie_data[ lie_data['TRUTH'] == 0 ].shape)\n",
    "\n",
    "print(\"\\n\\tTRAIN\")\n",
    "print(X_train.shape)\n",
    "print(\"TRUE :\", sum(y_train == 1 ) )\n",
    "print(\"FALSE:\", sum(y_train == 0 ) )\n",
    "\n",
    "print(\"\\n\\tTEST\")\n",
    "print(X_valid.shape)\n",
    "print(\"TRUE :\", sum(y_valid == 1 ) )\n",
    "print(\"FALSE:\", sum(y_valid == 0 ) )\n",
    "# X_train\n",
    "# X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65faa3-9f95-430a-8364-e5b0ff84d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7d303-27c9-427c-a195-c1cbf12c5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb355d-2e89-436d-8dd6-3fbe93fae650",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3f1ae-9c4e-43fc-a3dd-30c831cdb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience  = 5,\n",
    "    min_delta = 0.001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628e51c-f345-4ec9-a573-4411c30ba1ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size = 256,\n",
    "    epochs = 256,\n",
    "    # callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\", ylim=[-0.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0f32e-f2bb-4d09-9a93-1f6d6018175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c67cb0-1562-4f76-a7a6-1cf3b3d7c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "for i in df.values:  # iterate over the values\n",
    "    prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "    if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "        sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "random.shuffle(sequential_data)  # shuffle for good measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5ebf0-79f5-4288-ae25-03f404bdc2a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b03012-9a70-428b-99df-a1ab3a79abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "\n",
    "plt.savefig('b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d9b06-1779-4543-b2c7-4c9169bf8737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
