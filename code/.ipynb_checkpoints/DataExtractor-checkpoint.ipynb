{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649261b2-4240-4bf2-9050-9e5f0eb227e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From N:\\Mayur\\Programming\\GitHub\\lie-detection\\code\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading as thd\n",
    "\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b946a-07b6-47ac-b284-4a5612f3ded1",
   "metadata": {},
   "source": [
    "#### Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cada68-f2ab-4ffb-9603-c2040a9db5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(image:np.array, title=\"image\"):\n",
    "    cv2.imshow(title, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c3673-999b-4d82-b92e-a9233251603f",
   "metadata": {},
   "source": [
    "## Face bounds detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed7364a-ec0a-4641-8613-9f5b3713b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE\n",
    "        )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "        \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            self.draw_bounds(image, bbox)\n",
    "            return self.expand_bounds(bbox, scale=2)      # double width and height\n",
    "\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=2)\n",
    "        x = max(0, bbox.origin_x - bbox.width*(scale-1)//2  )\n",
    "        y = max(0, bbox.origin_y - bbox.height*(scale-1)//2 )\n",
    "\n",
    "        print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*bbox.width)\n",
    "        bbox.height = round(scale*bbox.height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb126-0278-4405-b71b-43ccff4eff1f",
   "metadata": {},
   "source": [
    "## Facemesh detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0601fa25-4324-4487-b734-d05519bfa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh()\n",
    "\n",
    "    def detect(self, image):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3736b-d21b-4654-a3c5-3b4489dad2a8",
   "metadata": {},
   "source": [
    "## 2) Live video detecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08712012-09d9-4f09-8b5a-63edf2f12ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8997ddc6-d9a8-4c1a-8f7b-78c81b7332d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 35\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "#cam = cv2.VideoCapture(r\".\\previous-MLcode\\Train_video.mp4\")\n",
    "\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = cam.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    if not _: break\n",
    "\n",
    "    cropx256 = cv2.resize(frame[bbox.y : bbox.y+bbox.width, bbox.x : bbox.x+bbox.height], (256,256))\n",
    "    \n",
    "    results = face_mesh_detector.detect(cropx256)\n",
    "\n",
    "    cv2.imshow(\"cam\", cropx256)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
