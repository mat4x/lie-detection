{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126b946a-07b6-47ac-b284-4a5612f3ded1",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649261b2-4240-4bf2-9050-9e5f0eb227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading as thd\n",
    "\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf904b0-f569-4982-8583-2e71f00fa86b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c3673-999b-4d82-b92e-a9233251603f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Face bounds detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7364a-ec0a-4641-8613-9f5b3713b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "    \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            self.draw_bounds(image, bbox)\n",
    "            return self.expand_bounds(bbox, scale=2)      # double width and height\n",
    "\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=2):\n",
    "        height, width = bbox.height, bbox.width\n",
    "        x = max(0, round( bbox.origin_x -  width*(scale-1)/2 )  )\n",
    "        y = max(0, round( bbox.origin_y - height*(scale-1)/2  - height*0.3) )\n",
    "\n",
    "        #print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*width)\n",
    "        bbox.height = round(scale*height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb126-0278-4405-b71b-43ccff4eff1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Facemesh detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601fa25-4324-4487-b734-d05519bfa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.LEFT_IRIS  = [474,475, 476, 477]\n",
    "        self.RIGHT_IRIS = [469, 470, 471, 472]\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "    def detect(self, image:np.array):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "        cv2.polylines(image, [mesh_points[self.LEFT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "        cv2.polylines(image, [mesh_points[self.RIGHT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c39819-0e9d-4fc9-ab2e-5aa2818ebd15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Holistic detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1050-be6e-4ab8-8d88-12f34843b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolisticDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_holistic = mp.solutions.holistic\n",
    "        self.holistic    = self.mp_holistic.Holistic()\n",
    "\n",
    "    def detect(self, image:np.array):\n",
    "        results = self.holistic.process(image)\n",
    "        self.draw_landmarks(image, results)     # OPTIONAL\n",
    "        return results\n",
    "\n",
    "    def draw_landmarks(self, image:np.array, results):\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image = image,\n",
    "            landmark_list = results.pose_landmarks,\n",
    "            connections   = self.mp_holistic.POSE_CONNECTIONS,\n",
    "            \n",
    "            landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(0,230,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1),\n",
    "            \n",
    "            connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(255,255,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efccedb-2d59-421e-9cfa-a9a328989526",
   "metadata": {},
   "source": [
    "## Data Accumulater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330be730-7c59-42e2-82f6-60fb3bccdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- eyebrow (L&R)                x2\n",
    "- lips    (L&R)                x2\n",
    "- face: left right top bottom  x4\n",
    "- body                         x8   < change here\n",
    "- iris    (L&R)                x2\n",
    "= 18 columns : x and y         = 36 features\n",
    "'''\n",
    "\n",
    "columns0 = ['lip_L', 'lip_R', 'brow_L', 'brow_R', 'iris_L', 'iris_R'] + [f\"face{i}\" for i in range(4)]  + [f\"body{i}\" for i in range(8)]\n",
    "columns  = list()\n",
    "for col in columns0:\n",
    "    columns.append(col+'x')\n",
    "    columns.append(col+'y')\n",
    "columns.extend(['TRUTH', 'frame'])\n",
    "TRAINING_FEATURES = pd.DataFrame(columns=columns)\n",
    "TESTING_FEATURES  = pd.DataFrame(columns=columns)\n",
    "\n",
    "# https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "LANDMARKS_LOC = {\n",
    "    'brow_L'     : {107, 66, 105, 63, 70, 46, 53, 52, 65, 55},\n",
    "    'brow_R'     : {336, 285, 296, 295, 334, 282, 293, 283, 276, 300},\n",
    "    'lip_L'      : {78, 191, 80, 81, 82, 95, 88, 178, 87},\n",
    "    'lip_R'      : {308, 415, 324, 310, 318, 311, 402, 312, 317},\n",
    "    'face0'      : {54, 68, 103, 104, 108, 69, 67, 10, 151, 338, 337, 397, 333, 332, 298, 284, 251, 301, 21, 71, 109, 297, 299},                                                                       #forehead\n",
    "    'face1'      : {18, 32, 83, 140, 148, 152, 171, 175, 176, 199, 200, 201, 208, 262, 313, 369, 377, 396, 400, 421, 428},                                                       #chin\n",
    "    'face2'      : {36, 50, 58, 93, 101, 111, 116, 117, 118, 123, 132, 137, 138, 147, 172, 177, 186, 187, 192, 203, 205, 206, 207, 212, 213, 214, 215, 216, 227, 228, 234},      #left_face\n",
    "    'face3'      : {266, 280, 288, 323, 330, 340, 345, 346, 347, 352, 361, 366, 367, 376, 397, 401, 410, 411, 416, 423, 425, 426, 427, 432, 433, 434, 435, 436, 447, 448, 454}   #right_face\n",
    "}\n",
    "    \n",
    "# video intervals where the subject answers\n",
    "INTERVALS = (\n",
    "    (90,  140, 1),\n",
    "    (170, 220, 1),\n",
    "    (250, 340, 0),\n",
    "    (370, 450, 1),    # TESTING INTERVAL\n",
    "    (480, 540, 1),\n",
    "    (600, 660, 0),\n",
    "    (inf, inf,0) )    # prevents detection on remainder video\n",
    "\n",
    "TEST_INTERVAL = 4-1\n",
    "\n",
    "TRAINING_FEATURES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126cf7c-da96-49f3-8429-3d5a3d648fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureData:\n",
    "    def __init__(self, name=\"data\"):\n",
    "        self.name = name\n",
    "        self.df = pd.DataFrame(columns=columns)\n",
    "        self.df.set_index(\"frame\", inplace=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(self.df)\n",
    "        return self.name\n",
    "\n",
    "    def record_facelandmarks(self, frame_no, results):\n",
    "        pass\n",
    "\n",
    "    def record_pos(self, frame_no, results):\n",
    "        pass\n",
    "\n",
    "    def save_data(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb171eca-4ae8-4919-acb4-5b920a8374df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HERE\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[7,7,7,7]], columns=['a','b','c','d'], dtype=np.uint8)\n",
    "df.set_index('a', inplace=True)\n",
    "\n",
    "row = pd.Series({'b':2}, name=6)\n",
    "df2 = pd.concat([df, row.to_frame().T], axis=0)\n",
    "\n",
    "df2.loc[6] = [1,2,3]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d29790-b7d1-432f-8b13-2f7654b21940",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3736b-d21b-4654-a3c5-3b4489dad2a8",
   "metadata": {},
   "source": [
    "## Live video detecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08712012-09d9-4f09-8b5a-63edf2f12ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()\n",
    "holistic_detector  = HolisticDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a16f-4cd6-48e7-8e0c-bdb730bacfa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0a656e9-978e-483f-8d2f-8910049b0e5c",
   "metadata": {},
   "source": [
    "#cam = cv2.VideoCapture(0)\n",
    "#cam = cv2.VideoCapture(r\".\\previous-MLcode\\Train_video.mp4\")\n",
    "cam = cv2.VideoCapture(r\".\\train_videos\\train5-s.mp4\")    # 1 3 5 *4\n",
    "\n",
    "\n",
    "length = int(cv2.VideoCapture.get(cam, int(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(f\"Frames total count: {length}\")\n",
    "\n",
    "# get face bounds in image\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = cam.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "crop = { \"x1\" : bbox.y,                              # crop.x = bbox.y, because crop is matrix\n",
    "         \"x2\" : bbox.y+bbox.width,\n",
    "         \"y1\" : bbox.x,\n",
    "         \"y2\" : bbox.x+bbox.height }\n",
    "\n",
    "# process video\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    if not _: break\n",
    "\n",
    "    cropx256 = frame[crop[\"x1\"]:crop[\"x2\"], crop[\"y1\"]:crop[\"y2\"] ]\n",
    "    results  = face_mesh_detector.detect(cropx256)\n",
    "    \n",
    "    cropx256 = cv2.resize(cropx256, (512,512))\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"cam\", cropx256)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c666d-3b5d-4601-aa20-e685080f1bee",
   "metadata": {},
   "source": [
    "### Offset calculator test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f93f34a-5c35-4949-a301-1fb730e42cca",
   "metadata": {},
   "source": [
    "camF = cv2.VideoCapture(r\".\\train_videos\\train3-F.mp4\")\n",
    "camS = cv2.VideoCapture(r\".\\train_videos\\train3-s.mp4\")\n",
    "    \n",
    "offset = -117\n",
    "if offset>0:\n",
    "    camF.set(cv2.CAP_PROP_POS_FRAMES, offset-1)\n",
    "else:\n",
    "    camS.set(cv2.CAP_PROP_POS_FRAMES, -1*offset-1)\n",
    "\n",
    "for i in range(500):\n",
    "    cv2.imshow(\"front\", cv2.resize(camF.read()[1], (0,0), fx=0.6, fy=0.6))\n",
    "    cv2.imshow(\"side\" , cv2.resize(camS.read()[1], (0,0), fx=0.6, fy=0.6))\n",
    "    key = cv2.waitKey(200)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camF.release()\n",
    "camS.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b20506-8562-43a1-bd2d-5d19a1f52cc7",
   "metadata": {},
   "source": [
    "### CamConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d768a8-0c9a-476d-ba9e-3b36c0495588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamVideoManager:\n",
    "    def __init__(self, TRAIN_VIDEO_NO:int, camF, camS):\n",
    "        self.offsetF = self.offsetS = 0\n",
    "        self.camF = camF\n",
    "        self.camS = camS\n",
    "        \n",
    "        segments       = pd.read_csv(r\"./train_videos/segments.csv\", index_col=\"Train_no\")\n",
    "        self.selection = segments.loc[TRAIN_VID_NO]\n",
    "        offset = self.selection[\"offsetF\"]\n",
    "        if offset < 0:\n",
    "            self.offsetS = abs(offset)\n",
    "        else:\n",
    "            self.offsetF = abs(offset)\n",
    "\n",
    "        self.set_frame() # initialize\n",
    "        \n",
    "    \n",
    "    def get_intervals(self):\n",
    "        intervals = []\n",
    "        row = self.selection[2:].to_list()\n",
    "        for i in range(0, len(row), 3):\n",
    "            intervals.append((row[i], row[i+1], row[i+2]))\n",
    "        return(intervals)\n",
    "\n",
    "    \n",
    "    def set_frame(self, frame_no:int=0, cam:int=0):\n",
    "        if cam in {0,1}:\n",
    "            self.camF.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetF-1)\n",
    "        if cam in {0,2}:\n",
    "            self.camS.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetS-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a0679-d410-4c7a-a973-e5bd45893fa2",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa1b8a-85e1-44d2-9dae-d9ec3eeece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VID_NO = 3\n",
    "camF = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-f.mp4\")\n",
    "camS = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-s.mp4\")\n",
    "\n",
    "print( camS.get(cv2.CAP_PROP_FRAME_COUNT) )\n",
    "\n",
    "CAMS      = CamVideoManager(TRAIN_VID_NO, camF, camS)\n",
    "INTERVALS = CAMS.get_intervals()\n",
    "print(INTERVALS)\n",
    "\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = camF.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "crop = { \"x1\" : bbox.y,                                  # crop.x = bbox.y, because crop is matrix\n",
    "         \"x2\" : bbox.y+bbox.width,\n",
    "         \"y1\" : bbox.x,\n",
    "         \"y2\" : bbox.x+bbox.height }\n",
    "\n",
    "CAMS.set_frame(INTERVALS[0][0], 2)\n",
    "\n",
    "for i in range(500):\n",
    "    _, frameF = camF.read()\n",
    "    _, frameS = camS.read()\n",
    "\n",
    "\n",
    "    cropFx256 = frameF[crop[\"x1\"]:crop[\"x2\"], crop[\"y1\"]:crop[\"y2\"] ]\n",
    "    #results_face = face_mesh_detector.detect(cropFx256)\n",
    "    \n",
    "    #cropFx256  = cv2.resize(cropFx256, (512,512))\n",
    "\n",
    "    #results_holistic = holistic_detector.detect(frameS)\n",
    "    \n",
    "    cv2.imshow(\"front\", cropFx256)\n",
    "    cv2.imshow(\"side\" , cv2.resize(frameS, (0,0), fx=0.4, fy=0.4))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camF.release()\n",
    "camS.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbb8a9a1-956d-4f10-8b63-ee7a1c935581",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
