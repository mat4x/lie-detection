{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126b946a-07b6-47ac-b284-4a5612f3ded1",
   "metadata": {},
   "source": [
    "#### Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cada68-f2ab-4ffb-9603-c2040a9db5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not required\n",
    "def display(image:np.array, title=\"image\"):\n",
    "    cv2.imshow(title, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649261b2-4240-4bf2-9050-9e5f0eb227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading as thd\n",
    "\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c3673-999b-4d82-b92e-a9233251603f",
   "metadata": {},
   "source": [
    "## Face bounds detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7364a-ec0a-4641-8613-9f5b3713b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE\n",
    "        )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "        \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            self.draw_bounds(image, bbox)\n",
    "            return self.expand_bounds(bbox, scale=2)      # double width and height\n",
    "\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=2):\n",
    "        x = max(0, round( bbox.origin_x - bbox.width*(scale-1)/2 )  )\n",
    "        y = max(0, round( bbox.origin_y - bbox.height*(scale-1)/2 ) )\n",
    "\n",
    "        #print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*bbox.width)\n",
    "        bbox.height = round(scale*bbox.height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb126-0278-4405-b71b-43ccff4eff1f",
   "metadata": {},
   "source": [
    "## Facemesh detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601fa25-4324-4487-b734-d05519bfa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh()\n",
    "\n",
    "    def detect(self, image):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efccedb-2d59-421e-9cfa-a9a328989526",
   "metadata": {},
   "source": [
    "## Data Accumulater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330be730-7c59-42e2-82f6-60fb3bccdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- eyebrow (L&R)                x2\n",
    "- lips    (L&R)                x2\n",
    "- face: left right top bottom  x4\n",
    "- body                         x8   < change here\n",
    "- iris    (L&R)                x2\n",
    "= 18 columns : x and y         = 36 features\n",
    "'''\n",
    "\n",
    "columns0 = ['lip_L', 'lip_R', 'brow_L', 'brow_R', 'iris_L', 'iris_R'] + [f\"face{i}\" for i in range(4)]  + [f\"body{i}\" for i in range(8)]\n",
    "columns  = list()\n",
    "for col in columns0:\n",
    "    columns.append(col+'x')\n",
    "    columns.append(col+'y')\n",
    "columns.append('TRUTH')\n",
    "TRAINING_FEATURES = pd.DataFrame(columns=columns)\n",
    "TESTING_FEATURES  = pd.DataFrame(columns=columns)\n",
    "\n",
    "# https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "LANDMARKS_LOC = {\n",
    "    'brow_L'     : {107, 66, 105, 63, 70, 46, 53, 52, 65, 55},\n",
    "    'brow_R'     : {336, 285, 296, 295, 334, 282, 293, 283, 276, 300},\n",
    "    'lip_L'      : {78, 191, 80, 81, 82, 95, 88, 178, 87},\n",
    "    'lip_R'      : {308, 415, 324, 310, 318, 311, 402, 312, 317},\n",
    "    'face0'      : {54, 68, 103, 104, 108, 69, 67, 10, 151, 338, 337, 397, 333, 332, 298, 284, 251, 301, 21, 71, 109, 297, 299},                                                                       #forehead\n",
    "    'face1'      : {18, 32, 83, 140, 148, 152, 171, 175, 176, 199, 200, 201, 208, 262, 313, 369, 377, 396, 400, 421, 428},                                                       #chin\n",
    "    'face2'      : {36, 50, 58, 93, 101, 111, 116, 117, 118, 123, 132, 137, 138, 147, 172, 177, 186, 187, 192, 203, 205, 206, 207, 212, 213, 214, 215, 216, 227, 228, 234},      #left_face\n",
    "    'face3'      : {266, 280, 288, 323, 330, 340, 345, 346, 347, 352, 361, 366, 367, 376, 397, 401, 410, 411, 416, 423, 425, 426, 427, 432, 433, 434, 435, 436, 447, 448, 454}   #right_face\n",
    "}\n",
    "    \n",
    "# video intervals where the subject answers\n",
    "INTERVALS = (\n",
    "    (90,  140, 1),\n",
    "    (170, 220, 1),\n",
    "    (250, 340, 0),\n",
    "    (370, 450, 1),    # TESTING INTERVAL\n",
    "    (480, 540, 1),\n",
    "    (600, 660, 0),\n",
    "    (inf, inf,0) )    # prevents detection on remainder video\n",
    "\n",
    "TEST_INTERVAL = 4-1\n",
    "\n",
    "TRAINING_FEATURES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126cf7c-da96-49f3-8429-3d5a3d648fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaptureData:\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3736b-d21b-4654-a3c5-3b4489dad2a8",
   "metadata": {},
   "source": [
    "## 2) Live video detecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08712012-09d9-4f09-8b5a-63edf2f12ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997ddc6-d9a8-4c1a-8f7b-78c81b7332d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = cv2.VideoCapture(0)\n",
    "cam = cv2.VideoCapture(r\".\\previous-MLcode\\Train_video.mp4\")\n",
    "\n",
    "# get face bounds in image\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = cam.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "\n",
    "# process video\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    if not _: break\n",
    "\n",
    "    cropx256 = cv2.resize(frame[bbox.y : bbox.y+bbox.width, bbox.x : bbox.x+bbox.height], (512,512))\n",
    "    \n",
    "    results = face_mesh_detector.detect(cropx256)\n",
    "\n",
    "    cv2.imshow(\"cam\", cropx256)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
