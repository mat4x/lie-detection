{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b55bc23-ba66-444c-9dad-34c9e32d9015",
   "metadata": {},
   "source": [
    "# Lie Detection\n",
    "#### Based on Facial Micro-expressions, Body Posture, Eye Movements\n",
    "\n",
    "> Mayur Sharma\\\n",
    "> Rohan deep Kujur\\\n",
    "> Khushi Tulsian\\\n",
    "> Atharva Karve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b946a-07b6-47ac-b284-4a5612f3ded1",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649261b2-4240-4bf2-9050-9e5f0eb227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading as thd\n",
    "\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf904b0-f569-4982-8583-2e71f00fa86b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c3673-999b-4d82-b92e-a9233251603f",
   "metadata": {},
   "source": [
    "### Face bounds detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7364a-ec0a-4641-8613-9f5b3713b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "    \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            print(f\"Face in region: ({bbox.origin_x}, {bbox.origin_y}) \\t Width: {bbox.width}px Height:{bbox.height}px\")\n",
    "            \n",
    "            #self.draw_bounds(image, bbox)\n",
    "            \n",
    "            return self.expand_bounds(bbox)\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=1.3):\n",
    "        height, width = bbox.height, bbox.width\n",
    "        x = max(0, round( bbox.origin_x -  width*(scale-1)/2 )  )\n",
    "        y = max(0, round( bbox.origin_y - height*(scale-1)/2 - height*0.2) )\n",
    "\n",
    "        #print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*width)\n",
    "        bbox.height = round(scale*height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cb126-0278-4405-b71b-43ccff4eff1f",
   "metadata": {},
   "source": [
    "### Facemesh detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601fa25-4324-4487-b734-d05519bfa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.LEFT_IRIS  = list(LANDMARKS_LOC['iris_L'])\n",
    "        self.RIGHT_IRIS = list(LANDMARKS_LOC['iris_R'])\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "    \n",
    "    def detect(self, image:np.array):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "        cv2.polylines(image, [mesh_points[self.LEFT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "        cv2.polylines(image, [mesh_points[self.RIGHT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c39819-0e9d-4fc9-ab2e-5aa2818ebd15",
   "metadata": {},
   "source": [
    "### Holistic detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1050-be6e-4ab8-8d88-12f34843b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolisticDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_holistic = mp.solutions.holistic\n",
    "        self.holistic    = self.mp_holistic.Holistic()\n",
    "\n",
    "    \n",
    "    def detect(self, image:np.array):\n",
    "        results = self.holistic.process(image)\n",
    "        self.draw_landmarks(image, results)     # OPTIONAL\n",
    "    \n",
    "        return results\n",
    "\n",
    "    \n",
    "    def draw_landmarks(self, image:np.array, results):\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image = image,\n",
    "            landmark_list = results.pose_landmarks,\n",
    "            connections   = self.mp_holistic.POSE_CONNECTIONS,\n",
    "            \n",
    "            landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(0,230,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1),\n",
    "            \n",
    "            connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(255,255,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efccedb-2d59-421e-9cfa-a9a328989526",
   "metadata": {},
   "source": [
    "## Data Accumulater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330be730-7c59-42e2-82f6-60fb3bccdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- eyebrow (L&R)                x2\n",
    "- lips    (L&R)                x2\n",
    "- face: left right top bottom  x4\n",
    "- iris    (L&R)                x2\n",
    "- body                         x2   shoulder, knees\n",
    "- neck & body angle            x1\n",
    "= 13 columns : x and y         = 26 features\n",
    "'''\n",
    "\n",
    "columns0 = ['lip_L', 'lip_R', 'brow_L', 'brow_R', 'iris_L', 'iris_R'] + [f\"face{i}\" for i in range(4)] + [\"shoulder\", \"knee\"]\n",
    "columns  = list()\n",
    "for col in columns0:\n",
    "    columns.append(col+'x')\n",
    "    columns.append(col+'y')\n",
    "columns.extend(['neck_angle', 'waist_angle', 'frame', 'question_no', 'TRUTH'])\n",
    "\n",
    "\n",
    "# https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "# https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md\n",
    "LANDMARKS_LOC = {\n",
    "    'brow_L'     : {107, 66, 105, 63, 70, 46, 53, 52, 65, 55},\n",
    "    'brow_R'     : {336, 285, 296, 295, 334, 282, 293, 283, 276, 300},\n",
    "    'iris_L'     : {474, 475, 476, 477},\n",
    "    'iris_R'     : {469, 470, 471, 472},\n",
    "    'lip_L'      : {78, 191, 80, 81, 82, 95, 88, 178, 87},\n",
    "    'lip_R'      : {308, 415, 324, 310, 318, 311, 402, 312, 317},\n",
    "    'face0'      : {54, 68, 103, 104, 108, 69, 67, 10, 151, 338, 337, 397, 333, 332, 298, 284, 251, 301, 21, 71, 109, 297, 299},                                                                       #forehead\n",
    "    'face1'      : {18, 32, 83, 140, 148, 152, 171, 175, 176, 199, 200, 201, 208, 262, 313, 369, 377, 396, 400, 421, 428},                                                       #chin\n",
    "    'face2'      : {36, 50, 58, 93, 101, 111, 116, 117, 118, 123, 132, 137, 138, 147, 172, 177, 186, 187, 192, 203, 205, 206, 207, 212, 213, 214, 215, 216, 227, 228, 234},      #left_face\n",
    "    'face3'      : {266, 280, 288, 323, 330, 340, 345, 346, 347, 352, 361, 366, 367, 376, 397, 401, 410, 411, 416, 423, 425, 426, 427, 432, 433, 434, 435, 436, 447, 448, 454}   #right_face\n",
    "}\n",
    "POSTURE_LOC = {'nose':0, 'shoulder':12, 'elbow':14, 'knee':26} #nose, shoulder, elbow, knee\n",
    "\n",
    "\n",
    "# video intervals where the subject answers\n",
    "# INTERVALS = (\n",
    "#     (90,  140, 1),\n",
    "#     (170, 220, 1),\n",
    "#     (250, 340, 0),\n",
    "#     (370, 450, 1),    # TESTING INTERVAL\n",
    "#     (480, 540, 1),\n",
    "#     (600, 660, 0),\n",
    "#     (inf, inf,0) )    # prevents detection on remainder video\n",
    "\n",
    "# TEST_INTERVAL = 4-1\n",
    "\n",
    "# TRAINING_FEATURES = pd.DataFrame(columns=columns)\n",
    "# TESTING_FEATURES  = pd.DataFrame(columns=columns)\n",
    "# TRAINING_FEATURES.shape\n",
    "\n",
    "print(columns)\n",
    "pd.DataFrame(columns=columns).set_index(\"frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126cf7c-da96-49f3-8429-3d5a3d648fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureData:\n",
    "    def __init__(self, name=\"data\"):\n",
    "        self.name = name\n",
    "        \n",
    "        self.df_face = pd.DataFrame(columns=columns)\n",
    "        self.df_face.set_index(\"frame\", inplace=True)\n",
    "\n",
    "        self.df_pos = pd.DataFrame(columns=columns)\n",
    "        self.df_pos.set_index(\"frame\", inplace=True)\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        print(self.df)\n",
    "        return self.name\n",
    "\n",
    "    \n",
    "    def record_facelandmarks(self, frame_no, results, question_no, truth):\n",
    "        nose_fix = (0.5,0.5)\n",
    "        nose = results.multi_face_landmarks[0].landmark[4]  #nose at index 4\n",
    "        dist = np.array([nose.x-nose_fix[0], nose.y-nose_fix[1]])           #dist is for keeping the nose at (0.5,0.5)\n",
    "        features = {\"TRUTH\": truth, \"question_no\":question_no}\n",
    "        \n",
    "        for feature in LANDMARKS_LOC:\n",
    "            feature_loc = np.array([0,0], dtype=np.float64)\n",
    "            \n",
    "            for idx in LANDMARKS_LOC[feature]:\n",
    "                mark = results.multi_face_landmarks[0].landmark[idx]\n",
    "                feature_loc += np.array( [mark.x, mark.y] )\n",
    "                \n",
    "            feature_loc /= len(LANDMARKS_LOC[feature])           # average feature location\n",
    "            \n",
    "            result = feature_loc - dist                          # normalize\n",
    "            features[feature+'x'] = result[0]\n",
    "            features[feature+'y'] = result[1]\n",
    "        \n",
    "        new_row = pd.Series(features, name = frame_no)\n",
    "        self.df_face = pd.concat([self.df_face, new_row.to_frame().T])\n",
    "\n",
    "    \n",
    "    def calc_angle(self, p1, p2, p3):\n",
    "        a = p1[0] - p2[0], p1[1] - p2[1]\n",
    "        b = p1[0] - p3[0], p1[1] - p3[1]\n",
    "        ab = a[0]*b[0] + a[1]*b[1]\n",
    "\n",
    "        norm_a = math.sqrt(a[0]**2 + a[1]**2)\n",
    "        norm_b = math.sqrt(b[0]**2 + b[1]**2)\n",
    "\n",
    "        theta = round(math.degrees(math.acos(ab/(norm_a*norm_b))), 6)\n",
    "        return theta\n",
    "    \n",
    "    def record_posture(self, frame_no, results, question_no, truth):\n",
    "        hip_fix = (0.25, 0.55)\n",
    "        hip  = results.pose_landmarks.landmark[24]               #hip at index 24\n",
    "        dist = np.array([hip.x-hip_fix[0], hip.y-hip_fix[1]])    #dist is for keeping the hip at (0.25,0.55)\n",
    "        features = {\"TRUTH\": truth, \"question_no\":question_no}\n",
    "\n",
    "        locations = {}\n",
    "        \n",
    "        for feature in POSTURE_LOC:\n",
    "            idx  = POSTURE_LOC[feature]\n",
    "            mark = results.pose_landmarks.landmark[idx]\n",
    "            feature_loc = np.array([mark.x,mark.y], dtype=np.float64)\n",
    "            \n",
    "            result = feature_loc - dist                          #normalize\n",
    "            locations[feature] = (result[0], result[1])\n",
    "\n",
    "        \n",
    "        features[\"waist_angle\"] = self.calc_angle(hip_fix, locations[\"knee\"], locations[\"shoulder\"])\n",
    "        features[ \"neck_angle\"] = self.calc_angle(locations[\"shoulder\"], hip_fix, locations[\"nose\"])\n",
    "        \n",
    "        features[\"shoulderx\"]   = locations[\"shoulder\"][0]\n",
    "        features[\"shouldery\"]   = locations[\"shoulder\"][1]\n",
    "        features[\"kneex\"]       = locations[\"knee\"][0]\n",
    "        features[\"kneey\"]       = locations[\"knee\"][1]\n",
    "        \n",
    "        new_row = pd.Series(features, name = frame_no)\n",
    "        self.df_pos = pd.concat([self.df_pos, new_row.to_frame().T])\n",
    "\n",
    "    \n",
    "    def comiple_and_save_data(self):\n",
    "        df = pd.merge(self.df_pos, self.df_face, on='frame', how='inner')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffd246-cdaa-4713-b395-36b0429879ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CaptureData.calc_angle([0,0], [0,0], [0,1], [1,0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "793ceb39-0940-421e-8fe9-01307108c2ae",
   "metadata": {},
   "source": [
    "#  HERE\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[7,7,7,7]], columns=['a','b','c','d'], dtype=np.uint8)\n",
    "df.set_index('a', inplace=True)\n",
    "\n",
    "row = pd.Series({'b':2}, name=6)\n",
    "df2 = pd.concat([df, row.to_frame().T], axis=0)\n",
    "\n",
    "df2.loc[6] = [1,2,3]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3736b-d21b-4654-a3c5-3b4489dad2a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Live video detecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a16f-4cd6-48e7-8e0c-bdb730bacfa8",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0a656e9-978e-483f-8d2f-8910049b0e5c",
   "metadata": {},
   "source": [
    "#cam = cv2.VideoCapture(0)\n",
    "#cam = cv2.VideoCapture(r\".\\previous-MLcode\\Train_video.mp4\")\n",
    "cam = cv2.VideoCapture(r\".\\train_videos\\train5-s.mp4\")    # 1 3 5 *4\n",
    "\n",
    "\n",
    "length = int(cv2.VideoCapture.get(cam, int(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(f\"Frames total count: {length}\")\n",
    "\n",
    "# get face bounds in image\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = cam.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "crop = { \"x1\" : bbox.y,                              # crop.x = bbox.y, because crop is matrix\n",
    "         \"x2\" : bbox.y+bbox.width,\n",
    "         \"y1\" : bbox.x,\n",
    "         \"y2\" : bbox.x+bbox.height }\n",
    "\n",
    "# process video\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    if not _: break\n",
    "\n",
    "    cropx256 = frame[crop[\"x1\"]:crop[\"x2\"], crop[\"y1\"]:crop[\"y2\"] ]\n",
    "    results  = face_mesh_detector.detect(cropx256)\n",
    "    \n",
    "    cropx256 = cv2.resize(cropx256, (512,512))\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"cam\", cropx256)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c666d-3b5d-4601-aa20-e685080f1bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Offset calculator test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f93f34a-5c35-4949-a301-1fb730e42cca",
   "metadata": {},
   "source": [
    "camF = cv2.VideoCapture(r\".\\train_videos\\train3-F.mp4\")\n",
    "camS = cv2.VideoCapture(r\".\\train_videos\\train3-s.mp4\")\n",
    "    \n",
    "offset = -117\n",
    "if offset>0:\n",
    "    camF.set(cv2.CAP_PROP_POS_FRAMES, offset-1)\n",
    "else:\n",
    "    camS.set(cv2.CAP_PROP_POS_FRAMES, -1*offset-1)\n",
    "\n",
    "for i in range(500):\n",
    "    cv2.imshow(\"front\", cv2.resize(camF.read()[1], (0,0), fx=0.6, fy=0.6))\n",
    "    cv2.imshow(\"side\" , cv2.resize(camS.read()[1], (0,0), fx=0.6, fy=0.6))\n",
    "    key = cv2.waitKey(200)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camF.release()\n",
    "camS.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b20506-8562-43a1-bd2d-5d19a1f52cc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CamConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d768a8-0c9a-476d-ba9e-3b36c0495588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamVideoManager:\n",
    "    def __init__(self, TRAIN_VIDEO_NO:int, camF, camS):\n",
    "        self.offsetF   = self.offsetS = 0\n",
    "        self.camF      = camF\n",
    "        self.camS      = camS\n",
    "        \n",
    "        segments       = pd.read_csv(r\"./train_videos/segments.csv\", index_col=\"Train_no\")\n",
    "        self.selection = segments.loc[TRAIN_VID_NO]\n",
    "        offset         = self.selection[\"offsetF\"]\n",
    "        \n",
    "        if offset < 0:\n",
    "            self.offsetS = abs(offset)\n",
    "        else:\n",
    "            self.offsetF = abs(offset)\n",
    "\n",
    "        self.set_frame() # initialize\n",
    "        self.INTERVALS = self.get_intervals()\n",
    "        \n",
    "    \n",
    "    def get_intervals(self) -> list[dict]:\n",
    "        intervals = []\n",
    "        row = self.selection[3:].to_list()\n",
    "        for i in range(0, len(row), 3):\n",
    "            intervals.append( {\"start\": row[i], \"end\":row[i+1], \"truth\":row[i+2]} )\n",
    "        return(intervals)\n",
    "\n",
    "    \n",
    "    def set_frame(self, frame_no:int=0, cam:int=0):\n",
    "        #cam: 0 = both,   1 = front,   2 = side\n",
    "        if cam in {0,1}:\n",
    "            self.camF.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetF-1)\n",
    "        if cam in {0,2}:\n",
    "            self.camS.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetS-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a0679-d410-4c7a-a973-e5bd45893fa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Process_tasks (threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed6bcf-86af-4782-b5e8-4133c2a80a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_face(camF):\n",
    "    print(\"PROCESSING FACE\")\n",
    "    CURR_FRAME    = 1\n",
    "    CURR_INTERVAL = 0\n",
    "\n",
    "\n",
    "    bbox = None\n",
    "    while bbox is None:\n",
    "        _, frameF = camF.read()\n",
    "        bbox = face_bounds_detect.detect_face_bounds(frameF)\n",
    "    \n",
    "    CROP = { \"y1\" : bbox.y,\n",
    "             \"y2\" : bbox.y+bbox.width,\n",
    "             \"x1\" : bbox.x,\n",
    "             \"x2\" : bbox.x+bbox.height }\n",
    "\n",
    "\n",
    "    CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "    CAMS.set_frame(CURR_FRAME, 1)\n",
    "    \n",
    "    while True:\n",
    "        # print(CURR_FRAME)\n",
    "        _, frameF = camF.read()\n",
    "\n",
    "        cropFx256    = cv2.resize(frameF[CROP[\"y1\"]:CROP[\"y2\"], CROP[\"x1\"]:CROP[\"x2\"] ], (350,350))\n",
    "        results_face = face_mesh_detector.detect(cropFx256)\n",
    "        capture_data.record_facelandmarks(CURR_FRAME, results_face, CURR_INTERVAL, CAMS.INTERVALS[CURR_INTERVAL][\"truth\"])\n",
    "        \n",
    "        cv2.imshow(\"front\", cropFx256)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        CURR_FRAME += 1\n",
    "        if CURR_FRAME == CAMS.INTERVALS[CURR_INTERVAL]['end']:\n",
    "            if CURR_FRAME == LAST_FRAME:\n",
    "                cv2.destroyWindow(\"front\")\n",
    "                print(\"END OF STREAM\")\n",
    "                break\n",
    "            CURR_INTERVAL += 1\n",
    "            CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "            CAMS.set_frame(CURR_FRAME, 1)\n",
    "            print(\"JUMP\"*10, CURR_INTERVAL, CAMS.INTERVALS[CURR_INTERVAL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59afcf-578f-49bd-a4f4-0b33cdf917d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posture(camS):\n",
    "    print(\"PROCESSING BODY123456\")\n",
    "    CURR_FRAME    = 1\n",
    "    CURR_INTERVAL = 0\n",
    "\n",
    "    \n",
    "    bbox = None\n",
    "    while bbox is None:\n",
    "        _, frameS = camS.read()\n",
    "        bbox = holistic_detector.detect(frameS).pose_landmarks.landmark\n",
    "    \n",
    "    height, width, _ = frameS.shape\n",
    "    CROP = { \"y1\" : round(height*bbox[ 0].y -150),                                   # crop.x = bbox.y, because crop is matrix\n",
    "             \"y2\" : round(height*bbox[32].y +100),\n",
    "             \"x1\" : round( width*bbox[24].x -150),\n",
    "             \"x2\" : round( width*bbox[26].x +180) }\n",
    "    print(CROP)\n",
    "\n",
    "    CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "    CAMS.set_frame(CURR_FRAME, 2)\n",
    "    \n",
    "    while True:\n",
    "        # print(CURR_FRAME)\n",
    "        _, frameS = camS.read()\n",
    "        frameS = frameS[CROP[\"y1\"]:CROP[\"y2\"], CROP[\"x1\"]:CROP[\"x2\"] ]\n",
    "        \n",
    "        results_posture = holistic_detector.detect(frameS)\n",
    "        capture_data.record_posture(CURR_FRAME, results_posture, CURR_INTERVAL, CAMS.INTERVALS[CURR_INTERVAL][\"truth\"])\n",
    "        \n",
    "        cv2.imshow(\"side\", cv2.resize(frameS, (0,0), fx=0.4, fy=0.4))\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        CURR_FRAME += 1\n",
    "        if CURR_FRAME == CAMS.INTERVALS[CURR_INTERVAL]['end']:\n",
    "            if CURR_FRAME == LAST_FRAME:\n",
    "                cv2.destroyWindow(\"side\")\n",
    "                print(\"END OF STREAM\")\n",
    "                break\n",
    "            CURR_INTERVAL += 1\n",
    "            CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "            CAMS.set_frame(CURR_FRAME, 2)\n",
    "            print(\"JUMP\"*10, CURR_INTERVAL, CAMS.INTERVALS[CURR_INTERVAL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a91d4e-027b-4204-8f03-ebd93aad113b",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08712012-09d9-4f09-8b5a-63edf2f12ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VID_NO  = 3\n",
    "\n",
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()\n",
    "holistic_detector  = HolisticDetector()\n",
    "capture_data = CaptureData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0881ed-e148-42f5-ba43-4502313f94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "camF = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-f.mp4\")\n",
    "camS = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-s.mp4\")\n",
    "\n",
    "CAMS = CamVideoManager(TRAIN_VID_NO, camF, camS)\n",
    "CAMS.set_frame(CAMS.INTERVALS[0][\"start\"])\n",
    "LAST_FRAME = CAMS.INTERVALS[-1][\"end\"]\n",
    "print(f\"{CAMS.INTERVALS}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8faa243-ca2d-483c-a55a-2db4f67fc540",
   "metadata": {},
   "source": [
    "# get frame screenshots\n",
    "_, frame = camS.read()\n",
    "cv2.imwrite('frame.png', frame)\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d4e78-4439-46df-8735-335bd0c2ddea",
   "metadata": {},
   "source": [
    "## MultiThreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ff7a4-fd4b-4c4a-9bf8-c7521d167bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_posture(camS)\n",
    "# process_face(camF)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    thd1 = thd.Thread(target=process_posture, args=(camS,))\n",
    "    thd2 = thd.Thread(target=process_face,    args=(camF,))\n",
    "    \n",
    "    thd1.start()\n",
    "    thd2.start()\n",
    "    \n",
    "    thd1.join()\n",
    "    thd2.join()\n",
    "    print(\"DONE\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71185b5-ffba-48a0-be24-840a5a581225",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"khushi\"\n",
    "\n",
    "c1 = ['lip_Lx', 'lip_Ly', 'lip_Rx', 'lip_Ry', 'brow_Lx', 'brow_Ly', 'brow_Rx', 'brow_Ry', 'iris_Lx', 'iris_Ly', 'iris_Rx', 'iris_Ry', 'face0x', 'face0y', 'face1x', 'face1y', 'face2x', 'face2y', 'face3x', 'face3y']\n",
    "c2 = ['shoulderx', 'shouldery', 'kneex', 'kneey', 'neck_angle', 'waist_angle', 'question_no', 'TRUTH']\n",
    "\n",
    "d1 = capture_data.df_face.drop(columns=c2)\n",
    "d2 = capture_data.df_pos.drop( columns=c1)\n",
    "\n",
    "#capture_data.df_pos, capture_data.df_face, how=\"inner\", on=\"frame\")\n",
    "\n",
    "DATASET = pd.concat([d1,d2], axis=1)\n",
    "DATASET.to_csv(rf'.\\data\\{name}_landmarka.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f25c7e-ea8b-4695-8475-c9399b091684",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Synced (non-multi-thread)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25d25116-058c-49ba-9c86-8f2e2b942bf9",
   "metadata": {},
   "source": [
    "CURR_FRAME    = 1\n",
    "CURR_INTERVAL = 0\n",
    "\n",
    "CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "CAMS.set_frame(CURR_FRAME)\n",
    "\n",
    "\n",
    "while True:  # while True\n",
    "    #print(CURR_FRAME)\n",
    "    \n",
    "    _, frameF = camF.read()\n",
    "    _, frameS = camS.read()\n",
    "\n",
    "    cropFx256    = cv2.resize(frameF[CROP[\"x1\"]:CROP[\"x2\"], CROP[\"y1\"]:CROP[\"y2\"] ], (350, 350))\n",
    "    results_face = face_mesh_detector.detect(cropFx256)\n",
    "\n",
    "    #results_holistic = holistic_detector.detect(frameS)\n",
    "    \n",
    "    cv2.imshow(\"front\", cropFx256)\n",
    "    cv2.imshow(\"frontOg\", frameF)\n",
    "    cv2.imshow(\"side\" , cv2.resize(frameS, (0,0), fx=0.4, fy=0.4))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    CURR_FRAME += 1\n",
    "    if CURR_FRAME == CAMS.INTERVALS[CURR_INTERVAL]['end']:\n",
    "        if CURR_FRAME == LAST_FRAME:\n",
    "            print(\"END OF STREAM\")\n",
    "            break\n",
    "        CURR_INTERVAL += 1\n",
    "        CURR_FRAME = CAMS.INTERVALS[CURR_INTERVAL][\"start\"]\n",
    "        CAMS.set_frame(CURR_FRAME)\n",
    "        print(\"JUMP\"*10, CURR_INTERVAL, CAMS.INTERVALS[CURR_INTERVAL])\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camF.release()\n",
    "camS.release()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbb8a9a1-956d-4f10-8b63-ee7a1c935581",
   "metadata": {},
   "source": [
    "INTERVALS=[{'start': 137, 'end': 208, 'truth': 1}, {'start': 236, 'end': 310, 'truth': 1}, {'start': 348, 'end': 391, 'truth': 1}, {'start': 468, 'end': 545, 'truth': 1}, {'start': 843, 'end': 892, 'truth': 1}, {'start': 931, 'end': 1037, 'truth': 0}, {'start': 1111, 'end': 1146, 'truth': 1}, {'start': 1248, 'end': 1312, 'truth': 0}, {'start': 1345, 'end': 1453, 'truth': 1}, {'start': 1625, 'end': 1675, 'truth': 0}, {'start': 1764, 'end': 1843, 'truth': 1}, {'start': 1912, 'end': 1970, 'truth': 1}, {'start': 2051, 'end': 2145, 'truth': 1}, {'start': 2206, 'end': 2310, 'truth': 0}, {'start': 2356, 'end': 2451, 'truth': 1}]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c89ed42-f3cf-4f02-a3bb-6b5f945a1dbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
