{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ec45a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import inf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import threading as thd\n",
    "\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56564dc7-a1f1-4344-9d31-4e9401e61c4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "# left eyes landmarks\n",
    "LEFT_EYE  = [ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "# right eyes landmarks\n",
    "RIGHT_EYE = [ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ] \n",
    "\n",
    "LEFT_IRIS  = [474,475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "Left_eb = [107,66,105,63,70,46,53,52,65,55]\n",
    "\n",
    "\n",
    "######################\n",
    "#'.\\\\Data\\\\Train_video.mp4' r'.\\previous-MLcode\\Train_video.mp4'\n",
    "cap = cv2.VideoCapture(0)\n",
    "#DIMS  = np.array([460, 880], dtype=int)      #Video crop dimension to subject\n",
    "#SCALE = 0.75\n",
    "######################\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh:\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w = frame.shape[:2]\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        \n",
    "        ############################################\n",
    "        '''\n",
    "        frame = frame[300:300+DIMS[1], 80:150+DIMS[0]]\n",
    "        frame = cv.resize(frame, (0, 0), fx = SCALE, fy = SCALE)\n",
    "        frame = cv.flip(frame,0)\n",
    "        img_h, img_w = frame.shape[:2]\n",
    "        results = face_mesh.process(frame)\n",
    "        '''\n",
    "        ############################################\n",
    "        '''\n",
    "        for face_landmark in results.multi_face_landmarks: \n",
    "        \n",
    "            mp_drawing.draw_landmarks(\n",
    "                    image = frame,\n",
    "                    landmark_list = mesh_points,\n",
    "                    connections   = mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        \n",
    "                    landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                        color=(255,0,0),\n",
    "                        thickness=0,\n",
    "                        circle_radius=1),\n",
    "                    \n",
    "                    connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                        color=(255,255,255),\n",
    "                        thickness=1,\n",
    "                        circle_radius=1)    )\n",
    "        '''\n",
    "        if results.multi_face_landmarks:\n",
    "            #print(results.multi_face_landmarks[0].landmark)\n",
    "            \n",
    "            mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "            # multiplying with height and width to get the actual coordinates of the landmarks from {x:0.56 y:0.21} to 200, 300 etc.\n",
    "            \n",
    "            # print(mesh_points.shape)\n",
    "            \n",
    "            \n",
    "            cv2.polylines(frame, [mesh_points[LEFT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "            cv2.polylines(frame, [mesh_points[RIGHT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            #print(results.multi_face_landmarks[0].landmark)\n",
    "        cv2.imshow('img', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key ==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570ba86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = results.multi_face_landmarks[0].landmark[0]\n",
    "a.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee44676-b1d7-4907-ab07-d711439335e0",
   "metadata": {},
   "source": [
    "# Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b943ab8-cfa5-4de8-a940-2671bc582d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE\n",
    "        )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "        \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            self.draw_bounds(image, bbox)\n",
    "            return self.expand_bounds(bbox, scale=2)      # double width and height\n",
    "\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=2):\n",
    "        x = max(0, round( bbox.origin_x - bbox.width*(scale-1)/2 )  )\n",
    "        y = max(0, round( bbox.origin_y - bbox.height*(scale-1)/2 ) )\n",
    "\n",
    "        #print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*bbox.width)\n",
    "        bbox.height = round(scale*bbox.height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcd7a8-4d2d-4aac-8db2-fec88b896b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.LEFT_IRIS  = list(LANDMARKS_LOC['iris_L'])\n",
    "        self.RIGHT_IRIS = list(LANDMARKS_LOC['iris_R'])\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "    def detect(self, image:np.array):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "        cv2.polylines(image, [mesh_points[self.LEFT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "        cv2.polylines(image, [mesh_points[self.RIGHT_IRIS]], True, (0,255,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1475ef-081f-4bb6-83e5-0125559ac9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolisticDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_holistic = mp.solutions.holistic\n",
    "        self.holistic    = self.mp_holistic.Holistic()\n",
    "\n",
    "    def detect(self, image:np.array):\n",
    "        results = self.holistic.process(image)\n",
    "        self.draw_landmarks(image, results)     # OPTIONAL\n",
    "        return results\n",
    "\n",
    "    def draw_landmarks(self, image:np.array, results):\n",
    "        if not results.pose_landmarks:\n",
    "            return\n",
    "        h,w,_ = image.shape\n",
    "        \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image = image,\n",
    "            landmark_list = results.pose_landmarks,\n",
    "            connections   = self.mp_holistic.POSE_CONNECTIONS,\n",
    "            \n",
    "            landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(0,230,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1),\n",
    "            \n",
    "            connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                color=(255,255,255),\n",
    "                thickness=2,\n",
    "                circle_radius=1)\n",
    "            )\n",
    "        for id,lm in enumerate(results.pose_landmarks.landmark):\n",
    "            x = int(lm.x*w)\n",
    "            y = int(lm.y*h)\n",
    "            #print('in',x,y)\n",
    "            cv2.circle(image,(x,y),1,(255,0,255),-1)\n",
    "            cv2.putText(image,str(id),(x,y -1),cv2.FONT_HERSHEY_PLAIN,4,(200,200,200),4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f4781-bc1b-4b9b-9565-2f5b4724d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamVideoManager:\n",
    "    def __init__(self, TRAIN_VIDEO_NO:int, camF, camS):\n",
    "        self.offsetF = self.offsetS = 0\n",
    "        self.camF = camF\n",
    "        self.camS = camS\n",
    "        \n",
    "        segments       = pd.read_csv(r\"./train_videos/segments.csv\", index_col=\"Train_no\")\n",
    "        self.selection = segments.loc[TRAIN_VID_NO]\n",
    "        offset = self.selection[\"offsetF\"]\n",
    "        if offset < 0:\n",
    "            self.offsetS = abs(offset)\n",
    "        else:\n",
    "            self.offsetF = abs(offset)\n",
    "\n",
    "        self.set_frame() # initialize\n",
    "        \n",
    "    \n",
    "    def get_intervals(self):\n",
    "        intervals = []\n",
    "        row = self.selection[2:].to_list()\n",
    "        for i in range(0, len(row), 3):\n",
    "            intervals.append((row[i], row[i+1], row[i+2]))\n",
    "        return(intervals)\n",
    "\n",
    "    \n",
    "    def set_frame(self, frame_no:int=0, cam:int=0):\n",
    "        if cam in {0,1}:\n",
    "            self.camF.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetF-1)\n",
    "        if cam in {0,2}:\n",
    "            self.camS.set(cv2.CAP_PROP_POS_FRAMES, frame_no+self.offsetS-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a357a9-6379-4098-bab8-fa4fffead346",
   "metadata": {},
   "source": [
    "# Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade5027-2688-4b9a-9e02-6317594bfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- eyebrow (L&R)                x2\n",
    "- lips    (L&R)                x2\n",
    "- face: left right top bottom  x4\n",
    "- body                         x8   < change here\n",
    "- iris    (L&R)                x2\n",
    "= 18 columns : x and y         = 36 features\n",
    "'''\n",
    "\n",
    "columns0 = ['lip_L', 'lip_R', 'brow_L', 'brow_R', 'iris_L', 'iris_R'] + [f\"face{i}\" for i in range(4)]  + [f\"body{i}\" for i in range(4)]\n",
    "columns  = list()\n",
    "for col in columns0:\n",
    "    columns.append(col+'x')\n",
    "    columns.append(col+'y')\n",
    "columns.extend(['TRUTH', 'frame'])\n",
    "TRAINING_FEATURES = pd.DataFrame(columns=columns)\n",
    "TESTING_FEATURES  = pd.DataFrame(columns=columns)\n",
    "\n",
    "# https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "LANDMARKS_LOC = {\n",
    "    'brow_L'     : {107, 66, 105, 63, 70, 46, 53, 52, 65, 55},\n",
    "    'brow_R'     : {336, 285, 296, 295, 334, 282, 293, 283, 276, 300},\n",
    "    'iris_L'     : {474, 475, 476, 477},\n",
    "    'iris_R'     : {469, 470, 471, 472}, \n",
    "    'lip_L'      : {78, 191, 80, 81, 82, 95, 88, 178, 87},\n",
    "    'lip_R'      : {308, 415, 324, 310, 318, 311, 402, 312, 317},\n",
    "    'face0'      : {54, 68, 103, 104, 108, 69, 67, 10, 151, 338, 337, 397, 333, 332, 298, 284, 251, 301, 21, 71, 109, 297, 299},                                                                       #forehead\n",
    "    'face1'      : {18, 32, 83, 140, 148, 152, 171, 175, 176, 199, 200, 201, 208, 262, 313, 369, 377, 396, 400, 421, 428},                                                       #chin\n",
    "    'face2'      : {36, 50, 58, 93, 101, 111, 116, 117, 118, 123, 132, 137, 138, 147, 172, 177, 186, 187, 192, 203, 205, 206, 207, 212, 213, 214, 215, 216, 227, 228, 234},      #left_face\n",
    "    'face3'      : {266, 280, 288, 323, 330, 340, 345, 346, 347, 352, 361, 366, 367, 376, 397, 401, 410, 411, 416, 423, 425, 426, 427, 432, 433, 434, 435, 436, 447, 448, 454}   #right_face\n",
    "}\n",
    "POSTURE_LOC = {'body0':0, 'body1':12, 'body2':14, 'body3':26} #nose, shoulder, elbow, knee\n",
    "    \n",
    "# video intervals where the subject answers\n",
    "INTERVALS = (\n",
    "    (90,  140, 1),\n",
    "    (170, 220, 1),\n",
    "    (250, 340, 0),\n",
    "    (370, 450, 1),    # TESTING INTERVAL\n",
    "    (480, 540, 1),\n",
    "    (600, 660, 0),\n",
    "    (inf, inf,0) )    # prevents detection on remainder video\n",
    "\n",
    "TEST_INTERVAL = 4-1\n",
    "\n",
    "TRAINING_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515a97e-fffd-499c-aa01-82492f46c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureData:\n",
    "    def __init__(self, name=\"data\"):\n",
    "        self.name = name\n",
    "        self.df_face = pd.DataFrame(columns=columns)\n",
    "        self.df_face.set_index(\"frame\", inplace=True)\n",
    "        \n",
    "        self.df_pos = pd.DataFrame(columns=columns)\n",
    "        self.df_pos.set_index(\"frame\", inplace=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        print(self.df)\n",
    "        return self.name\n",
    "\n",
    "    def record_facelandmarks(self, frame_no, results):\n",
    "        nose = results.multi_face_landmarks[0].landmark[4]  #nose at index 4\n",
    "        #x, y = get_cordinates(nose, DIMS)\n",
    "        #cv2.circle(frame, (x, y), 2, (0,0,0), 1)\n",
    "        features = dict()\n",
    "        \n",
    "        for feature in LANDMARKS_LOC:\n",
    "            feature_loc = np.array([0,0], dtype=np.float64)\n",
    "            \n",
    "            for idx in LANDMARKS_LOC[feature]:\n",
    "                mark = results.multi_face_landmarks[0].landmark[idx]\n",
    "                feature_loc += np.array( [mark.x, mark.y] )\n",
    "                \n",
    "            feature_loc /= len(LANDMARKS_LOC[feature])           #average feature location\n",
    "            \n",
    "            result = feature_loc - np.array([nose.x, nose.y])    #normalize\n",
    "            features[feature+'x'] = result[0]\n",
    "            features[feature+'y'] = result[1]\n",
    "        \n",
    "        new_row = pd.Series(features, name = frame_no)\n",
    "        self.df_face = pd.concat([self.df_face, new_row.to_frame().T])\n",
    "        print(self.df_face)\n",
    "        pass\n",
    "\n",
    "    def record_pos(self, frame_no, results):\n",
    "        hip = results.pose_landmarks.landmark[24]  #hip at index 24\n",
    "        #x, y = get_cordinates(nose, DIMS)\n",
    "        #cv2.circle(frame, (x, y), 2, (0,0,0), 1)\n",
    "        features = dict()\n",
    "        \n",
    "        for feature in POSTURE_LOC:\n",
    "            idx = POSTURE_LOC[feature]\n",
    "            mark = results.pose_landmarks.landmark[idx]\n",
    "            feature_loc = np.array([mark.x,mark.y], dtype=np.float64)\n",
    "            \n",
    "            result = feature_loc - np.array([hip.x, hip.y])    #normalize\n",
    "            features[feature+'x'] = result[0]\n",
    "            features[feature+'y'] = result[1]\n",
    "        \n",
    "        new_row = pd.Series(features, name = frame_no)\n",
    "        self.df_pos = pd.concat([self.df_pos, new_row.to_frame().T])\n",
    "        print(self.df_pos)\n",
    "        pass\n",
    "\n",
    "    def save_data(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb18d28-dfec-4e38-8c67-d5ca71ccab89",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b421d-9c04-4a3e-ae8b-bbfe271b8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()\n",
    "capture_data = CaptureData()\n",
    "holistic_detector  = HolisticDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b869e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_VID_NO = 1\n",
    "camF = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-f.mp4\")\n",
    "camS = cv2.VideoCapture(rf\".\\train_videos\\train{TRAIN_VID_NO}-s.mp4\")\n",
    "\n",
    "#print( camS.get(cv2.CAP_PROP_FRAME_COUNT) )\n",
    "\n",
    "CAMS      = CamVideoManager(TRAIN_VID_NO, camF, camS)\n",
    "INTERVALS = CAMS.get_intervals()\n",
    "#print(INTERVALS)\n",
    "\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = camF.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "crop = { \"x1\" : bbox.y,                                  # crop.x = bbox.y, because crop is matrix\n",
    "         \"x2\" : bbox.y+bbox.width,\n",
    "         \"y1\" : bbox.x,\n",
    "         \"y2\" : bbox.x+bbox.height }\n",
    "\n",
    "CAMS.set_frame(INTERVALS[0][0])\n",
    "\n",
    "for i in range(20):\n",
    "    _, frameF = camF.read()\n",
    "    _, frameS = camS.read()\n",
    "\n",
    "\n",
    "    cropFx256 = frameF[crop[\"x1\"]:crop[\"x2\"], crop[\"y1\"]:crop[\"y2\"] ]\n",
    "    results_face = face_mesh_detector.detect(cropFx256)\n",
    "    capture_data.record_facelandmarks(i,results_face) #record face data\n",
    "    cropFx256  = cv2.resize(cropFx256, (512,512))\n",
    "\n",
    "    results_holistic = holistic_detector.detect(frameS)\n",
    "    capture_data.record_pos(i,results_holistic) #record posture data\n",
    "    \n",
    "    cv2.imshow(\"front\", cropFx256)\n",
    "    cv2.imshow(\"side\" , cv2.resize(frameS, (0,0), fx=0.4, fy=0.4))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camF.release()\n",
    "camS.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16218a-c1a5-48a8-9b0b-adfef12d3229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f842224-b5b2-4a35-840f-2c9adb3df779",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.results_holistic.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245571d-bf04-4088-9926-959eec389e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in LANDMARKS_LOC:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cc000-46b2-4525-a3ef-17e04fb8cd37",
   "metadata": {},
   "source": [
    "features value 1_frame:\n",
    "{'brow_Lx': -0.06354176104068754, 'brow_Ly': -0.0748566150665283, 'brow_Rx': 0.07440438866615295, 'brow_Ry': -0.08823261857032777, 'iris_lx': 0.07112482190132141, 'iris_ly': -0.046568095684051514, 'iris_rx': -0.05164928734302521, 'iris_ry': -0.03550254553556442, 'lip_Lx': -0.01693820291095305, 'lip_Ly': 0.06732548938857186, 'lip_Rx': 0.04028668668535018, 'lip_Ry': 0.06273240513271755, 'face0x': 0.011293470859527588, 'face0y': -0.08929244331691577, 'face1x': 0.012932631231489689, 'face1y': 0.13295339260782513, 'face2x': -0.08397503245261406, 'face2y': 0.046176983464148724, 'face3x': 0.11203789903271588, 'face3y': 0.0337283505547431}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbbc51-6027-4422-9c13-58009b81ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_face.multi_face_landmarks[0].landmark[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c13f26-57c3-44b7-ab3b-53088a2a7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = {'A': [1, 2, 3],\n",
    "            'B': ['a', 'd', 'c'],\n",
    "            'C': [55, 21, 32]}\n",
    "# Existing DataFrame\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "# New row to add\n",
    "new_row = pd.DataFrame({'B': 4,\n",
    "                        'A': ['d']})\n",
    "\n",
    "# Concatenate the existing DataFrame and the new row\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa876c22-c84e-483d-8f93-e12e9117a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame({'B': 55,\n",
    "                        'A': ['d']})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193bbcf-c3d2-4d6d-a45a-a60a9bd93713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "capture_data.df_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c8e87-e33e-48f4-aaf2-2e71fc9c9fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9d2ee-e32f-4aa7-beb3-0cc3263f9ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
