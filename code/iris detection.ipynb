{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9ec45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import rshift\n",
    "import cv2 as cv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp \n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing  = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Github_repos\\lie-detection\\code\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left eyes landmarks\n",
    "LEFT_EYE  = [ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "# right eyes landmarks\n",
    "RIGHT_EYE = [ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ] \n",
    "\n",
    "LEFT_IRIS  = [474,475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "######################\n",
    "#'.\\\\Data\\\\Train_video.mp4'\n",
    "cap = cv.VideoCapture(r'.\\previous-MLcode\\Train_video.mp4')\n",
    "#DIMS  = np.array([460, 880], dtype=int)      #Video crop dimension to subject\n",
    "#SCALE = 0.75\n",
    "######################\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh:\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv.flip(frame, 1)\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img_h, img_w = frame.shape[:2]\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        \n",
    "        ############################################\n",
    "        '''\n",
    "        frame = frame[300:300+DIMS[1], 80:150+DIMS[0]]\n",
    "        frame = cv.resize(frame, (0, 0), fx = SCALE, fy = SCALE)\n",
    "        frame = cv.flip(frame,0)\n",
    "        img_h, img_w = frame.shape[:2]\n",
    "        results = face_mesh.process(frame)\n",
    "        '''\n",
    "        ############################################\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            #print(results.multi_face_landmarks[0].landmark)\n",
    "            \n",
    "            mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "            # multiplying with height and width to get the actual coordinates of the landmarks from {x:0.56 y:0.21} to 200, 300 etc.\n",
    "            \n",
    "            # print(mesh_points.shape)\n",
    "            cv.polylines(frame, [mesh_points[LEFT_IRIS]], True, (0,255,0), 1, cv.LINE_AA)\n",
    "            cv.polylines(frame, [mesh_points[RIGHT_IRIS]], True, (0,255,0), 1, cv.LINE_AA)\n",
    "            '''\n",
    "            (l_cx, l_cy), l_radius = cv.minEnclosingCircle(mesh_points[LEFT_IRIS])\n",
    "            (r_cx, r_cy), r_radius = cv.minEnclosingCircle(mesh_points[RIGHT_IRIS])\n",
    "            center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "            center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "            cv.circle(frame, center_left, int(l_radius), (255,0,255), 1, cv.LINE_AA)\n",
    "            cv.circle(frame, center_right, int(r_radius), (255,0,255), 1, cv.LINE_AA)\n",
    "            '''\n",
    "            #print(results.multi_face_landmarks[0].landmark)\n",
    "        cv.imshow('img', frame)\n",
    "        key = cv.waitKey(1)\n",
    "        if key ==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3570ba86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = results.multi_face_landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee44676-b1d7-4907-ab07-d711439335e0",
   "metadata": {},
   "source": [
    "# XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b943ab8-cfa5-4de8-a940-2671bc582d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html\n",
    "\n",
    "class FaceDetector:\n",
    "    '''\n",
    "    FaceDetector is used to get the 'bounds' for a face.\n",
    "    'bounds' are used to crop the image befor eseonding it for face/iris landmark detection.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # creating detector object\n",
    "        with open(r\".\\data\\blaze_face_short_range.tflite\", \"rb\") as model_file:\n",
    "            model_data = model_file.read()\n",
    "        \n",
    "        options = vision.FaceDetectorOptions(\n",
    "            base_options = python.BaseOptions(model_asset_buffer=model_data),\n",
    "            running_mode = vision.RunningMode.IMAGE\n",
    "        )\n",
    "        \n",
    "        self.face_detector = vision.FaceDetector.create_from_options(options)\n",
    "        \n",
    "    def detect_face_bounds(self, image:np.array) -> tuple:\n",
    "        # convert nump image to mediapipe format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # detect face in image\n",
    "        self.face_detection_result = self.face_detector.detect(mp_image)\n",
    "    \n",
    "        # if face detected, draw on image and return bounds\n",
    "        if self.face_detection_result.detections:\n",
    "            bbox = self.face_detection_result.detections[0].bounding_box\n",
    "            self.draw_bounds(image, bbox)\n",
    "            return self.expand_bounds(bbox, scale=1)      # double width and height\n",
    "\n",
    "        print(\"No face detected\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def draw_bounds(self, image:np.array, bbox) -> tuple:\n",
    "        height, width, _ = image.shape\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point   = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255,0,0), 3)\n",
    "\n",
    "    \n",
    "    def expand_bounds(self, bbox, scale:int=2):\n",
    "        x = max(0, round( bbox.origin_x - bbox.width*(scale-1)/2 )  )\n",
    "        y = max(0, round( bbox.origin_y - bbox.height*(scale-1)/2 ) )\n",
    "\n",
    "        print(x,y)\n",
    "        bbox.x = x\n",
    "        bbox.y = y\n",
    "        bbox.width  = round(scale*bbox.width)\n",
    "        bbox.height = round(scale*bbox.height)\n",
    "\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd254b51-cb12-46c8-b7c7-d4754a06e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMeshDetector:\n",
    "    def __init__(self):\n",
    "        self.LEFT_IRIS  = [474,475, 476, 477]\n",
    "        self.RIGHT_IRIS = [469, 470, 471, 472]\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh    = self.mp_face_mesh.FaceMesh()\n",
    "\n",
    "    def detect(self, image):\n",
    "        results = self.face_mesh.process(image)\n",
    "        self.draw_face_landmarks(image, results)   # OPTIONAL\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_face_landmarks(self, image, results):\n",
    "        if not results.multi_face_landmarks:\n",
    "            return\n",
    "        #####wrute here\n",
    "        for face_landmark in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image = image,\n",
    "                landmark_list = face_landmark,\n",
    "                connections   = self.mp_face_mesh.FACEMESH_TESSELATION,\n",
    "    \n",
    "                landmark_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,0,0),\n",
    "                    thickness=0,\n",
    "                    circle_radius=1),\n",
    "                \n",
    "                connection_drawing_spec = mp_drawing.DrawingSpec(\n",
    "                    color=(255,255,255),\n",
    "                    thickness=1,\n",
    "                    circle_radius=1)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97b421d-9c04-4a3e-ae8b-bbfe271b8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_bounds_detect = FaceDetector()\n",
    "face_mesh_detector = FaceMeshDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b869e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 376\n"
     ]
    }
   ],
   "source": [
    "#cam = cv2.VideoCapture(0)\n",
    "cam = cv2.VideoCapture(r\".\\previous-MLcode\\Train_video.mp4\")\n",
    "\n",
    "# get face bounds in image\n",
    "bbox = None\n",
    "while bbox is None:\n",
    "    _, frame = cam.read()\n",
    "    bbox = face_bounds_detect.detect_face_bounds(frame)  # keep this before while True\n",
    "\n",
    "\n",
    "# process video\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    if not _: break\n",
    "\n",
    "    cropx256 = cv2.resize(frame[bbox.y : bbox.y+bbox.width, bbox.x : bbox.x+bbox.height], (256,256))\n",
    "    #put results = iris.detect(cropx256)\n",
    "    results = face_mesh_detector.detect(cropx256)\n",
    "\n",
    "    cv2.imshow(\"cam\", cropx256)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e37689-a286-4d8f-9315-f6e7cabd1b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
